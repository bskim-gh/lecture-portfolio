"""
ì‹¤ì „ ì˜ˆì œ 2: ì›¹ ê²€ìƒ‰ ë° ë°ì´í„° ìˆ˜ì§‘ MCP ì„œë²„
- ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
- HTML íŒŒì‹±
- ê°„ë‹¨í•œ ë°ì´í„° ì¶”ì¶œ
- API í˜¸ì¶œ
"""

from fastmcp import FastMCP
from typing import Dict, List
import json

# MCP ì„œë²„ ìƒì„±
mcp = FastMCP(
    name="ì›¹ ê²€ìƒ‰ MCP ì„œë²„",
    version="1.0.0"
)

# ============================================
# 1. ì›¹ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
# ============================================

@mcp.tool()
def fetch_url(url: str) -> str:
    """
    URLì˜ HTML ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬
    
    Args:
        url: ê°€ì ¸ì˜¬ ì›¹ í˜ì´ì§€ URL
    
    Returns:
        HTML ë‚´ìš© (ì²˜ìŒ 1000ì)
    """
    try:
        import requests
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # ì²˜ìŒ 1000ìë§Œ ë°˜í™˜
        content = response.text[:1000]
        return f"âœ… í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì„±ê³µ\n\në‚´ìš© (ì²˜ìŒ 1000ì):\n{content}..."
    
    except requests.RequestException as e:
        return f"âŒ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}"
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}"

@mcp.tool()
def get_page_title(url: str) -> str:
    """
    ì›¹ í˜ì´ì§€ì˜ ì œëª©ì„ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
    
    Returns:
        í˜ì´ì§€ ì œëª©
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        title = soup.find('title')
        
        if title:
            return f"ğŸ“„ í˜ì´ì§€ ì œëª©: {title.text.strip()}"
        else:
            return "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}"

# ============================================
# 2. ë°ì´í„° ì¶”ì¶œ
# ============================================

@mcp.tool()
def extract_links(url: str) -> str:
    """
    ì›¹ í˜ì´ì§€ì—ì„œ ëª¨ë“  ë§í¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ë„êµ¬
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
    
    Returns:
        ë§í¬ ëª©ë¡
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        links = soup.find_all('a', href=True)
        
        result = f"ğŸ”— ë°œê²¬ëœ ë§í¬ ({len(links)}ê°œ):\n\n"
        
        for i, link in enumerate(links[:20], 1):  # ìµœëŒ€ 20ê°œ
            href = urljoin(url, link['href'])
            text = link.text.strip() or "(í…ìŠ¤íŠ¸ ì—†ìŒ)"
            result += f"{i}. {text}\n   {href}\n\n"
        
        if len(links) > 20:
            result += f"... ì™¸ {len(links) - 20}ê°œ ë”"
        
        return result
    
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}"

@mcp.tool()
def extract_images(url: str) -> str:
    """
    ì›¹ í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•˜ëŠ” ë„êµ¬
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
    
    Returns:
        ì´ë¯¸ì§€ URL ëª©ë¡
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        images = soup.find_all('img', src=True)
        
        result = f"ğŸ–¼ï¸ ë°œê²¬ëœ ì´ë¯¸ì§€ ({len(images)}ê°œ):\n\n"
        
        for i, img in enumerate(images[:15], 1):  # ìµœëŒ€ 15ê°œ
            src = urljoin(url, img['src'])
            alt = img.get('alt', '(ì„¤ëª… ì—†ìŒ)')
            result += f"{i}. {alt}\n   {src}\n\n"
        
        if len(images) > 15:
            result += f"... ì™¸ {len(images) - 15}ê°œ ë”"
        
        return result
    
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}"

# ============================================
# 3. API í˜¸ì¶œ
# ============================================

@mcp.tool()
def get_json_data(url: str) -> Dict:
    """
    JSON APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬
    
    Args:
        url: API URL
    
    Returns:
        JSON ë°ì´í„°
    """
    try:
        import requests
        
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        return {"success": True, "data": data}
    
    except Exception as e:
        return {"success": False, "error": str(e)}

@mcp.tool()
def check_url_status(url: str) -> Dict:
    """
    URLì˜ ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ë„êµ¬
    
    Args:
        url: í™•ì¸í•  URL
    
    Returns:
        ìƒíƒœ ì •ë³´
    """
    try:
        import requests
        
        response = requests.head(url, timeout=10, allow_redirects=True)
        
        info = {
            "url": url,
            "status_code": response.status_code,
            "status": "ì •ìƒ" if response.status_code == 200 else "ë¬¸ì œ ë°œìƒ",
            "content_type": response.headers.get('Content-Type', 'Unknown'),
            "server": response.headers.get('Server', 'Unknown')
        }
        
        return info
    
    except Exception as e:
        return {"url": url, "error": str(e)}

# ============================================
# 4. ìœ ìš©í•œ ë„êµ¬ë“¤
# ============================================

@mcp.tool()
def search_in_page(url: str, keyword: str) -> str:
    """
    ì›¹ í˜ì´ì§€ì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ëŠ” ë„êµ¬
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
        keyword: ê²€ìƒ‰í•  í‚¤ì›Œë“œ
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text()
        
        count = text.lower().count(keyword.lower())
        
        if count > 0:
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ ì°¾ê¸°
            sentences = text.split('.')
            matches = [s.strip() for s in sentences if keyword.lower() in s.lower()]
            
            result = f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼: {count}íšŒ ë°œê²¬\n\n"
            result += "ê´€ë ¨ ë¬¸ì¥:\n"
            
            for i, match in enumerate(matches[:5], 1):
                result += f"{i}. {match[:100]}...\n\n"
            
            return result
        else:
            return f"'{keyword}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {str(e)}"

@mcp.tool()
def get_page_metadata(url: str) -> Dict:
    """
    ì›¹ í˜ì´ì§€ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë„êµ¬
    
    Args:
        url: ì›¹ í˜ì´ì§€ URL
    
    Returns:
        ë©”íƒ€ë°ì´í„° ì •ë³´
    """
    try:
        import requests
        from bs4 import BeautifulSoup
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        metadata = {
            "url": url,
            "title": soup.find('title').text if soup.find('title') else "ì—†ìŒ",
            "description": "",
            "keywords": "",
            "author": ""
        }
        
        # ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
        for meta in soup.find_all('meta'):
            name = meta.get('name', '').lower()
            property_attr = meta.get('property', '').lower()
            content = meta.get('content', '')
            
            if name == 'description' or property_attr == 'og:description':
                metadata['description'] = content
            elif name == 'keywords':
                metadata['keywords'] = content
            elif name == 'author':
                metadata['author'] = content
        
        return metadata
    
    except Exception as e:
        return {"error": str(e)}

# ============================================
# Main ì‹¤í–‰
# ============================================
if __name__ == "__main__":
    import os
    
    print("\n" + "=" * 60)
    print("  ì›¹ ê²€ìƒ‰ MCP ì„œë²„")
    print("=" * 60)
    
    print("\në“±ë¡ëœ Tools:")
    tools = [
        "fetch_url - URL ë‚´ìš© ê°€ì ¸ì˜¤ê¸°",
        "get_page_title - í˜ì´ì§€ ì œëª© ê°€ì ¸ì˜¤ê¸°",
        "extract_links - ë§í¬ ì¶”ì¶œ",
        "extract_images - ì´ë¯¸ì§€ URL ì¶”ì¶œ",
        "get_json_data - JSON API í˜¸ì¶œ",
        "check_url_status - URL ìƒíƒœ í™•ì¸",
        "search_in_page - í˜ì´ì§€ ë‚´ í‚¤ì›Œë“œ ê²€ìƒ‰",
        "get_page_metadata - ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"
    ]
    
    for i, tool in enumerate(tools, 1):
        print(f"  {i}. {tool}")
    
    print("\n" + "=" * 60)
    print("  í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬")
    print("=" * 60)
    
    print("\npip install requests beautifulsoup4")
    
    print("\n" + "=" * 60)
    print("  Cursor ì„¤ì •")
    print("=" * 60)
    
    config = {
        "mcpServers": {
            "web-scraper": {
                "command": "python",
                "args": [os.path.abspath(__file__)]
            }
        }
    }
    print(json.dumps(config, indent=2, ensure_ascii=False))
    
    print("\n" + "=" * 60)
    print("  ì‚¬ìš© ì˜ˆì‹œ (Cursor ì±„íŒ…)")
    print("=" * 60)
    
    examples = """
    - "example.comì˜ ì œëª©ì„ ì•Œë ¤ì¤˜"
    - "ì´ í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ë¥¼ ì¶”ì¶œí•´ì¤˜"
    - "í˜ì´ì§€ì—ì„œ 'Python' í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•´ì¤˜"
    - "URL ìƒíƒœë¥¼ í™•ì¸í•´ì¤˜"
    """
    
    print(examples)

"""
ì£¼ì˜ì‚¬í•­:
- robots.txt í™•ì¸ í•„ìˆ˜
- ì›¹ì‚¬ì´íŠ¸ ì´ìš©ì•½ê´€ ì¤€ìˆ˜
- ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€
- User-Agent ì„¤ì • ê¶Œì¥

ì‚¬ìš© ì˜ˆì‹œ:
ì‚¬ìš©ì: "https://example.comì˜ ì œëª©ì„ ì•Œë ¤ì¤˜"
AI: [get_page_title í˜¸ì¶œ] ì œëª©: Example Domain
"""

