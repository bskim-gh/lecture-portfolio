"""
ì‹¤ì „ ì˜ˆì œ 1: ë‰´ìŠ¤ ìˆ˜ì§‘ í”„ë¡œê·¸ë¨
- ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ì œëª©, ë§í¬, ë‚ ì§œ ìˆ˜ì§‘
- ì—¬ëŸ¬ í˜ì´ì§€ ìˆœíšŒ
- CSV íŒŒì¼ë¡œ ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import csv
import time
from datetime import datetime
from urllib.parse import urljoin

class NewsCrawler:
    """
    ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
    """
    def __init__(self, base_url):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        self.articles = []
    
    def fetch_page(self, url):
        """
        í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        """
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {url}")
            print(f"   ì—ëŸ¬: {e}")
            return None
    
    def parse_article_list(self, html):
        """
        ê¸°ì‚¬ ëª©ë¡ íŒŒì‹± (ì˜ˆì œ)
        ì‹¤ì œ ì‚¬ì´íŠ¸ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        """
        soup = BeautifulSoup(html, 'html.parser')
        
        # ì˜ˆì œ HTML êµ¬ì¡° (ì‹¤ì œ ì‚¬ì´íŠ¸ì— ë§ê²Œ ìˆ˜ì •)
        articles = []
        
        # ê°€ìƒì˜ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ (ì‹¤ì œë¡œëŠ” soup.find_all()ë¡œ ì°¾ê¸°)
        # article_divs = soup.find_all('div', class_='article-item')
        
        # ë°ëª¨ìš© ë°ì´í„°
        for i in range(5):
            article = {
                'title': f'ë‰´ìŠ¤ ì œëª© {i+1}',
                'link': f'{self.base_url}/article/{i+1}',
                'date': datetime.now().strftime('%Y-%m-%d'),
                'summary': f'ë‰´ìŠ¤ ìš”ì•½ ë‚´ìš© {i+1}...'
            }
            articles.append(article)
        
        return articles
    
    def crawl_page(self, page_num):
        """
        íŠ¹ì • í˜ì´ì§€ í¬ë¡¤ë§
        """
        url = f"{self.base_url}?page={page_num}"
        print(f"\nğŸ“° í˜ì´ì§€ {page_num} í¬ë¡¤ë§ ì¤‘...")
        print(f"   URL: {url}")
        
        html = self.fetch_page(url)
        if not html:
            return []
        
        articles = self.parse_article_list(html)
        print(f"   âœ… {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        
        return articles
    
    def crawl_multiple_pages(self, start_page, end_page):
        """
        ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§
        """
        print("=" * 60)
        print("  ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 60)
        
        for page in range(start_page, end_page + 1):
            articles = self.crawl_page(page)
            self.articles.extend(articles)
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            if page < end_page:
                time.sleep(1)
        
        print(f"\nğŸ“Š ì´ {len(self.articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    
    def save_to_csv(self, filename):
        """
        CSV íŒŒì¼ë¡œ ì €ì¥
        """
        if not self.articles:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['title', 'link', 'date', 'summary']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            
            writer.writeheader()
            writer.writerows(self.articles)
        
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")
    
    def print_summary(self):
        """
        ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
        """
        print("\n" + "=" * 60)
        print("  ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        if not self.articles:
            print("ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print("\nìµœê·¼ 5ê°œ ê¸°ì‚¬:")
        for i, article in enumerate(self.articles[:5], 1):
            print(f"\n{i}. {article['title']}")
            print(f"   ë‚ ì§œ: {article['date']}")
            print(f"   ë§í¬: {article['link']}")
    
    def close(self):
        """
        ì„¸ì…˜ ì¢…ë£Œ
        """
        self.session.close()


# ============================================
# ì‹¤í–‰ ì˜ˆì œ
# ============================================
def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("\n" + "=" * 60)
    print("  ë‰´ìŠ¤ ìˆ˜ì§‘ í”„ë¡œê·¸ë¨")
    print("=" * 60)
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    base_url = "https://news.example.com"  # ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë¡œ ë³€ê²½
    crawler = NewsCrawler(base_url)
    
    try:
        # 1~3í˜ì´ì§€ í¬ë¡¤ë§
        crawler.crawl_multiple_pages(start_page=1, end_page=3)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        crawler.print_summary()
        
        # CSV ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'news_{timestamp}.csv'
        crawler.save_to_csv(filename)
        
    finally:
        crawler.close()
    
    print("\n" + "=" * 60)
    print("  í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("=" * 60)


if __name__ == "__main__":
    main()


"""
ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:

1. ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§:
   - URL íŒ¨í„´ ë¶„ì„
   - ê¸°ì‚¬ ì œëª©: div.news_tit ë˜ëŠ” a.news_tit
   - ë‚ ì§œ: span.info
   - robots.txt í™•ì¸ í•„ìˆ˜

2. ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§:
   - URL: https://news.daum.net/...
   - ê¸°ì‚¬ êµ¬ì¡° ë¶„ì„
   - CSS ì„ íƒì ì°¾ê¸°

ì£¼ì˜ì‚¬í•­:
- ë°˜ë“œì‹œ robots.txt í™•ì¸
- ì €ì‘ê¶Œ ì¤€ìˆ˜
- ì ì ˆí•œ ëŒ€ê¸° ì‹œê°„ ì„¤ì • (ì„œë²„ ë¶€í•˜ ë°©ì§€)
- User-Agent ì„¤ì •
- ì´ìš©ì•½ê´€ í™•ì¸

ì»¤ìŠ¤í„°ë§ˆì´ì§•:
1. parse_article_list() í•¨ìˆ˜ë¥¼ ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
2. CSS ì„ íƒì ë˜ëŠ” XPath ì‚¬ìš©
3. ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (ì‘ì„±ì, ì¡°íšŒìˆ˜ ë“±)
4. ë°ì´í„° ì •ì œ ë¡œì§ ì¶”ê°€
"""

