"""
ì‹¤ì „ ì˜ˆì œ 3: ë™ì  ì›¹ í˜ì´ì§€ í¬ë¡¤ë§ (Selenium)
- JavaScriptë¡œ ìƒì„±ë˜ëŠ” ì½˜í…ì¸  ìˆ˜ì§‘
- ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
- ë¡œê·¸ì¸ ìë™í™”
- ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import time
import json
from datetime import datetime

class DynamicWebCrawler:
    """
    ë™ì  ì›¹ í˜ì´ì§€ í¬ë¡¤ëŸ¬
    """
    def __init__(self, headless=False):
        self.options = Options()
        
        if headless:
            self.options.add_argument('--headless')
        
        # ê¸°ë³¸ ì˜µì…˜
        self.options.add_argument('--window-size=1920,1080')
        self.options.add_argument('--no-sandbox')
        self.options.add_argument('--disable-dev-shm-usage')
        self.options.add_argument('--disable-notifications')
        self.options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)')
        
        # ìë™í™” ê°ì§€ ìš°íšŒ
        self.options.add_experimental_option("excludeSwitches", ["enable-automation"])
        self.options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = None
        self.wait = None
        self.collected_data = []
    
    def start(self):
        """
        ë“œë¼ì´ë²„ ì‹œì‘
        """
        print("\nğŸš€ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        
        try:
            self.driver = webdriver.Chrome(options=self.options)
            self.driver.implicitly_wait(10)
            self.wait = WebDriverWait(self.driver, 15)
            print("âœ… ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨: {e}")
            print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
            print("   1. ChromeDriver ì„¤ì¹˜ í™•ì¸")
            print("   2. pip install webdriver-manager")
            print("   3. Chrome ë¸Œë¼ìš°ì € ìµœì‹  ë²„ì „ í™•ì¸")
            raise
    
    def stop(self):
        """
        ë“œë¼ì´ë²„ ì¢…ë£Œ
        """
        if self.driver:
            self.driver.quit()
            print("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ")
    
    def login(self, login_url, username, password):
        """
        ë¡œê·¸ì¸ ìë™í™” (ì˜ˆì œ)
        """
        print(f"\nğŸ” ë¡œê·¸ì¸ ì‹œë„: {login_url}")
        
        try:
            self.driver.get(login_url)
            time.sleep(2)
            
            # ì•„ì´ë”” ì…ë ¥ (ì‹¤ì œ ì„ íƒìë¡œ ë³€ê²½ í•„ìš”)
            username_input = self.wait.until(
                EC.presence_of_element_located((By.ID, "username"))
            )
            username_input.clear()
            username_input.send_keys(username)
            
            # ë¹„ë°€ë²ˆí˜¸ ì…ë ¥
            password_input = self.driver.find_element(By.ID, "password")
            password_input.clear()
            password_input.send_keys(password)
            
            # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
            login_button = self.driver.find_element(By.ID, "login-button")
            login_button.click()
            
            # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°
            time.sleep(3)
            
            print("âœ… ë¡œê·¸ì¸ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")
            return False
    
    def scroll_to_bottom(self, pause_time=2):
        """
        í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤
        """
        print("\nğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
        
        last_height = self.driver.execute_script(
            "return document.body.scrollHeight"
        )
        scroll_count = 0
        
        while True:
            # ìŠ¤í¬ë¡¤ ë‹¤ìš´
            self.driver.execute_script(
                "window.scrollTo(0, document.body.scrollHeight);"
            )
            scroll_count += 1
            print(f"   ìŠ¤í¬ë¡¤ {scroll_count}íšŒ")
            
            # ë¡œë”© ëŒ€ê¸°
            time.sleep(pause_time)
            
            # ìƒˆ ë†’ì´ ê³„ì‚°
            new_height = self.driver.execute_script(
                "return document.body.scrollHeight"
            )
            
            # ë” ì´ìƒ ìŠ¤í¬ë¡¤í•  ìˆ˜ ì—†ìœ¼ë©´ ì¢…ë£Œ
            if new_height == last_height:
                print(f"âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ (ì´ {scroll_count}íšŒ)")
                break
            
            last_height = new_height
    
    def scroll_slowly(self, scroll_pause=0.5):
        """
        ì²œì²œíˆ ìŠ¤í¬ë¡¤ (ìì—°ìŠ¤ëŸ¬ìš´ ë™ì‘)
        """
        print("\nğŸ“œ ì²œì²œíˆ ìŠ¤í¬ë¡¤ ì¤‘...")
        
        # ì „ì²´ ë†’ì´ ê°€ì ¸ì˜¤ê¸°
        total_height = self.driver.execute_script(
            "return document.body.scrollHeight"
        )
        
        # ë‹¨ê³„ë³„ë¡œ ìŠ¤í¬ë¡¤
        viewport_height = self.driver.execute_script(
            "return window.innerHeight"
        )
        
        current_position = 0
        while current_position < total_height:
            # í•œ í™”ë©´ì”© ìŠ¤í¬ë¡¤
            current_position += viewport_height
            self.driver.execute_script(
                f"window.scrollTo(0, {current_position});"
            )
            time.sleep(scroll_pause)
        
        print("âœ… ìŠ¤í¬ë¡¤ ì™„ë£Œ")
    
    def wait_and_click(self, by, value):
        """
        ìš”ì†Œ ëŒ€ê¸° í›„ í´ë¦­
        """
        try:
            element = self.wait.until(
                EC.element_to_be_clickable((by, value))
            )
            element.click()
            return True
        except Exception as e:
            print(f"âŒ í´ë¦­ ì‹¤íŒ¨: {e}")
            return False
    
    def extract_items(self, selector, max_items=None):
        """
        ì•„ì´í…œ ì¶”ì¶œ
        """
        print(f"\nğŸ“¦ ì•„ì´í…œ ì¶”ì¶œ ì¤‘... (ì„ íƒì: {selector})")
        
        try:
            # ìš”ì†Œ ë¡œë”© ëŒ€ê¸°
            self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            
            # ëª¨ë“  ìš”ì†Œ ì°¾ê¸°
            elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
            
            if max_items:
                elements = elements[:max_items]
            
            items = []
            for i, element in enumerate(elements, 1):
                try:
                    item = {
                        'index': i,
                        'text': element.text,
                        'html': element.get_attribute('innerHTML')
                    }
                    items.append(item)
                except:
                    continue
            
            print(f"âœ… {len(items)}ê°œ ì•„ì´í…œ ì¶”ì¶œ ì™„ë£Œ")
            return items
            
        except Exception as e:
            print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def extract_social_media_posts(self):
        """
        ì†Œì…œ ë¯¸ë””ì–´ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì˜ˆì œ (ê°€ìƒ)
        """
        print("\n" + "=" * 60)
        print("  ì†Œì…œ ë¯¸ë””ì–´ ê²Œì‹œë¬¼ ìˆ˜ì§‘")
        print("=" * 60)
        
        # ì˜ˆì œ URL (ì‹¤ì œ ì‚¬ìš© ì‹œ ë³€ê²½)
        url = "https://example-social.com"
        
        print(f"\nğŸŒ í˜ì´ì§€ ì ‘ì†: {url}")
        self.driver.get(url)
        time.sleep(3)
        
        # ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ë” ë§ì€ ê²Œì‹œë¬¼ ë¡œë“œ
        self.scroll_to_bottom(pause_time=2)
        
        # ê²Œì‹œë¬¼ ì¶”ì¶œ (ì‹¤ì œ ì„ íƒìë¡œ ë³€ê²½ í•„ìš”)
        posts = []
        
        # ë°ëª¨ ë°ì´í„°
        for i in range(10):
            post = {
                'id': i + 1,
                'author': f'ì‚¬ìš©ì{i+1}',
                'content': f'ê²Œì‹œë¬¼ ë‚´ìš© {i+1}...',
                'likes': (i + 1) * 10,
                'comments': (i + 1) * 5,
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            posts.append(post)
        
        self.collected_data.extend(posts)
        print(f"\nâœ… {len(posts)}ê°œ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return posts
    
    def take_screenshot(self, filename=None):
        """
        ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        """
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'screenshot_{timestamp}.png'
        
        self.driver.save_screenshot(filename)
        print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {filename}")
    
    def save_to_json(self, filename):
        """
        JSON íŒŒì¼ë¡œ ì €ì¥
        """
        if not self.collected_data:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        data = {
            'crawled_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_count': len(self.collected_data),
            'data': self.collected_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {filename}")
    
    def print_summary(self):
        """
        ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
        """
        print("\n" + "=" * 60)
        print("  ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        if not self.collected_data:
            print("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ“Š ì´ ë°ì´í„° ìˆ˜: {len(self.collected_data)}ê°œ")
        
        print("\nìµœê·¼ 5ê°œ ë°ì´í„°:")
        for item in self.collected_data[:5]:
            print(f"\n  â€¢ {item}")


# ============================================
# ì‹¤í–‰ ì˜ˆì œ
# ============================================
def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("\n" + "=" * 60)
    print("  ë™ì  ì›¹ í˜ì´ì§€ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨")
    print("=" * 60)
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = DynamicWebCrawler(headless=False)  # headless=Trueë¡œ ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    
    try:
        # ë“œë¼ì´ë²„ ì‹œì‘
        crawler.start()
        
        # ì˜ˆì œ 1: ì†Œì…œ ë¯¸ë””ì–´ ê²Œì‹œë¬¼ ìˆ˜ì§‘
        crawler.extract_social_media_posts()
        
        # ê²°ê³¼ ìš”ì•½
        crawler.print_summary()
        
        # JSON ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'dynamic_crawling_{timestamp}.json'
        crawler.save_to_json(filename)
        
        # ìŠ¤í¬ë¦°ìƒ·
        crawler.take_screenshot()
        
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        crawler.stop()
    
    print("\n" + "=" * 60)
    print("  í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("=" * 60)


if __name__ == "__main__":
    main()


"""
ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ:

1. ì¸ìŠ¤íƒ€ê·¸ë¨ í•´ì‹œíƒœê·¸ ìˆ˜ì§‘:
   - ë¡œê·¸ì¸ í•„ìš”
   - ë¬´í•œ ìŠ¤í¬ë¡¤ ì²˜ë¦¬
   - ê²Œì‹œë¬¼ ì •ë³´ ìˆ˜ì§‘
   
2. YouTube ë™ì˜ìƒ ì •ë³´:
   - ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ì§‘
   - ìŠ¤í¬ë¡¤ë¡œ ë” ë§ì€ ê²°ê³¼ ë¡œë“œ
   - ì œëª©, ì¡°íšŒìˆ˜, ì—…ë¡œë“œ ë‚ ì§œ ë“±

3. LinkedIn ì±„ìš© ê³µê³ :
   - ë¡œê·¸ì¸ í›„ ì ‘ê·¼
   - í•„í„° ì„¤ì •
   - ê³µê³  ì •ë³´ ìˆ˜ì§‘

ì£¼ì˜ì‚¬í•­:
- ê° ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ í™•ì¸ í•„ìˆ˜
- ë¡œê·¸ì¸ ì •ë³´ ë³´ì•ˆ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
- robots.txt í™•ì¸
- ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€
- API ì œê³µ ì—¬ë¶€ í™•ì¸ (API ì‚¬ìš© ê¶Œì¥)

ë³´ì•ˆ:
- ë¹„ë°€ë²ˆí˜¸ í•˜ë“œì½”ë”© ê¸ˆì§€
- í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ ì‚¬ìš©
- .gitignoreì— ì„¤ì • íŒŒì¼ ì¶”ê°€

ì»¤ìŠ¤í„°ë§ˆì´ì§•:
1. ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì— ë§ê²Œ ì„ íƒì ìˆ˜ì •
2. ë¡œê·¸ì¸ ë¡œì§ êµ¬í˜„
3. ë°ì´í„° ì •ì œ ë¡œì§ ì¶”ê°€
4. ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
5. ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
6. í”„ë¡ì‹œ ì‚¬ìš© (í•„ìš” ì‹œ)
"""

